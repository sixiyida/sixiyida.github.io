[{"content":"spring-ai-alibaba-graph-core 源码阅读 1 StateGraph 1.1 基础结构 1 2 3 4 5 6 7 8 9 10 public class StateGraph { // 核心数据结构 final Nodes nodes = new Nodes(); // 存储所有节点 final Edges edges = new Edges(); // 存储所有边 // 特殊节点常量 public static final String END = \u0026#34;__END__\u0026#34;; // 结束节点 public static final String START = \u0026#34;__START__\u0026#34;; // 起始节点 public static final String ERROR = \u0026#34;__ERROR__\u0026#34;; // 错误节点 } 1.2 构造方法 1 public StateGraph(String name, KeyStrategyFactory keyStrategyFactory, PlainTextStateSerializer stateSerializer) 参数必选KeyStrategyFactory，其他可选，序列化默认JacksonSerializer()。\n1.3 节点管理 节点具体实现请见2\n1 2 3 4 5 6 7 8 public static class Nodes { public final Set\u0026lt;Node\u0026gt; elements; // 节点集合 // 节点操作方法 public boolean anyMatchById(String id) // 检查节点是否存在 public List\u0026lt;SubStateGraphNode\u0026gt; onlySubStateGraphNodes() // 获取子图节点 public List\u0026lt;Node\u0026gt; exceptSubStateGraphNodes() // 获取非子图节点 } 1.4 边管理 1 2 3 4 5 6 7 public static class Edges { public final List\u0026lt;Edge\u0026gt; elements; // 边集合 // 边操作方法 public Optional\u0026lt;Edge\u0026gt; edgeBySourceId(String sourceId) // 根据源节点查找边 public List\u0026lt;Edge\u0026gt; edgesByTargetId(String targetId) // 根据目标节点查找边 } 1.5 添加节点 1 2 3 4 5 6 // 添加普通节点 public StateGraph addNode(String id, AsyncNodeAction action) // 添加带配置的节点 public StateGraph addNode(String id, AsyncNodeActionWithConfig actionWithConfig) // 添加子图节点 public StateGraph addNode(String id, StateGraph subGraph) 1.6 添加边 1 2 3 4 // 添加普通边 public StateGraph addEdge(String sourceId, String targetId) // 添加条件边 public StateGraph addConditionalEdges(String sourceId, AsyncCommandAction condition, Map\u0026lt;String, String\u0026gt; mappings) 1.7 图验证、编译和可视化 1 2 3 4 5 6 // 验证图的正确性 void validateGraph() throws GraphStateException // 编译图 public CompiledGraph compile(CompileConfig config) throws GraphStateException // 可视化 public GraphRepresentation getGraph(GraphRepresentation.Type type, String title) 1.8 序列化器 1 2 3 static class JacksonSerializer extends JacksonStateSerializer static class GsonSerializer extends GsonStateSerializer 1.9 状态管理 1 2 3 4 // 状态工厂 private OverAllStateFactory overAllStateFactory; // 键策略工厂 private KeyStrategyFactory keyStrategyFactory; 2 Node 2.1 Node基础节点 1 2 3 4 5 6 7 8 9 public class Node { private final String id; // 节点唯一标识 private final ActionFactory actionFactory; // 动作工厂 // 动作工厂接口 public interface ActionFactory { AsyncNodeActionWithConfig apply(CompileConfig config) throws GraphStateException; } } 2.2 ParalellNode 1 2 3 4 5 6 7 8 9 10 11 12 13 14 public class ParallelNode extends Node { public static final String PARALLEL_PREFIX = \u0026#34;__PARALLEL__\u0026#34;; // 并行动作实现 record AsyncParallelNodeAction( List\u0026lt;AsyncNodeActionWithConfig\u0026gt; actions, // 并行执行的动作列表 Map\u0026lt;String, KeyStrategy\u0026gt; channels // 通道策略 ) implements AsyncNodeActionWithConfig { // 并行执行所有动作 public CompletableFuture\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; apply(OverAllState state, RunnableConfig config) { // 使用 CompletableFuture 实现并行执行 } } } 待补充\n2.3 子图节点 2.3.1 子图节点接口 1 2 3 4 5 6 7 public interface SubGraphNode { String PREFIX_FORMAT = \u0026#34;%s-%s\u0026#34;; // 节点ID格式化模板 String id(); // 获取节点ID StateGraph subGraph(); // 获取子图 String formatId(String nodeId); // 格式化节点ID } 2.3.2 状态图子图节点 1 2 3 4 5 6 7 8 public class SubStateGraphNode extends Node implements SubGraphNode { private final StateGraph subGraph; // 子图 // 格式化节点ID public String formatId(String nodeId) { return SubGraphNode.formatId(id(), nodeId); } } 2.3.3 编译后的子图节点 1 2 3 4 5 6 7 8 public class SubCompiledGraphNode extends Node implements SubGraphNode { private final CompiledGraph subGraph; // 编译后的子图 public SubCompiledGraphNode(String id, CompiledGraph subGraph) { super(id, (config) -\u0026gt; new SubCompiledGraphNodeAction(subGraph)); this.subGraph = subGraph; } } 3 Edge 3.1 基础边Edge 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 public record Edge(String sourceId, List\u0026lt;EdgeValue\u0026gt; targets) { // 构造函数 public Edge(String sourceId, EdgeValue target) { this(sourceId, List.of(target)); } // 判断是否为并行边 public boolean isParallel() { return targets.size() \u0026gt; 1; } // 验证边的有效性 public void validate(StateGraph.Nodes nodes) throws GraphStateException { // 验证源节点存在 // 验证目标节点存在 // 验证并行边的目标不重复 } } 3.2 EdgeCondition 1 2 3 4 5 6 public record EdgeCondition( AsyncCommandAction action, // 异步命令动作 Map\u0026lt;String, String\u0026gt; mappings // 条件映射 ) { // 条件执行逻辑 } 3.3 EdgeValue 1 2 3 4 5 6 7 8 9 10 11 public record EdgeValue(String id, EdgeCondition value) { // 简单边值（只有ID） public EdgeValue(String id) { this(id, null); } // 条件边值（只有条件） public EdgeValue(EdgeCondition value) { this(null, value); } } 4 OverAllState OverAllState 是状态管理的核心，它贯穿整个图的执行过程。\n所有的 Action 都需要依赖状态来执行和传递数据。\n4.1 核心数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 public final class OverAllState implements Serializable { // 状态数据存储 private final Map\u0026lt;String, Object\u0026gt; data; // 键策略映射 private final Map\u0026lt;String, KeyStrategy\u0026gt; keyStrategies; // 恢复标志 private Boolean resume; // 人工反馈 private HumanFeedback humanFeedback; // 中断消息 private String interruptMessage; } 4.2 状态控制 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 public void cover(OverAllState overAllState){ // 替换所有数据 } public OverAllState input(Map\u0026lt;String, Object\u0026gt; input) { // input是null或空直接返回 // 使用keyStrategies操作key对应value Map\u0026lt;String, KeyStrategy\u0026gt; keyStrategies = keyStrategies(); input.keySet().stream().filter(key -\u0026gt; keyStrategies.containsKey(key)).forEach(key -\u0026gt; { this.data.put(key, keyStrategies.get(key).apply(value(key, null), input.get(key))); }); return this; } public Map\u0026lt;String, Object\u0026gt; updateState(Map\u0026lt;String, Object\u0026gt; partialState) { // 用partialState更新状态，和input一样 } public static Map\u0026lt;String, Object\u0026gt; updateState(Map\u0026lt;String, Object\u0026gt; state, Map\u0026lt;String, Object\u0026gt; partialState, Map\u0026lt;String, KeyStrategy\u0026gt; keyStrategies){ // 同上 } private static Map\u0026lt;String, Object\u0026gt; updatePartialStateFromSchema(Map\u0026lt;String, Object\u0026gt; state, Map\u0026lt;String, Object\u0026gt; partialState, Map\u0026lt;String, KeyStrategy\u0026gt; keyStrategies){ // 返回更新后的partialState但不更新状态 } ","date":"2025-06-06T00:00:00Z","permalink":"https://sixiyida.github.io/p/spring-ai-alibaba-graph-core-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/","title":"spring-ai-alibaba-graph-core 源码阅读"},{"content":"spring-ai-alibaba-graph-core 源码阅读 ","date":"2025-06-06T00:00:00Z","permalink":"https://sixiyida.github.io/p/spring-ai-alibaba%E7%BB%B4%E6%8A%A4%E8%AE%B0%E5%BD%95/","title":"spring-ai-alibaba维护记录"},{"content":"分库分表和分页 1. 分表（Table Sharding） 定义：将单张数据表按特定规则（如哈希、范围）拆分为多个结构相同的小表，存储在同一数据库或不同数据库中\n目的：解决单表数据量过大导致的查询性能下降（如索引膨胀、磁盘I/O瓶颈）\n适用场景：单表数据超千万级，但数据库实例资源未达瓶颈\n2. 分库（Database Sharding） 定义：将整个数据库按业务或数据维度拆分为多个独立的数据库实例，每个实例存储部分数据\n目的：解决单库连接数不足、磁盘空间不足、写并发压力大等问题\n适用场景：单库QPS过高、连接数耗尽或需故障隔离\n3. 分片（Sharding） 定义：分库+分表的组合策略，将数据按规则（如哈希、范围）分布到多个数据库节点（分片），每个节点包含部分库和表。\n目的：实现真正的水平扩展，支持海量数据与高并发\n适用场景：超大规模数据（TB/PB级）、需全局负载均衡\n核心区别总结 维度 分表 分库 分片 拆分对象 单张表 整个数据库实例 库+表组合的分布式节点 主要目标 解决单表性能瓶颈 解决单库资源瓶颈 全局水平扩展与高可用 数据分布 表内数据拆分 库间数据隔离 跨节点数据分片 典型场景 大表查询优化 高并发写入/连接数不足 超大规模系统（如社交平台） ","date":"2025-06-02T00:00:00Z","permalink":"https://sixiyida.github.io/p/%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/","title":"系统设计"},{"content":"0. 前言 花了5天时间是跟着做完了黑马点评项目。虽说是烂大街项目之一，但我还是学到了不少东西。这个项目让我第一次看到了后端的全貌。这篇文章会记录我优化这个项目的过程，涉及中间件、功能拓展、LLM的引入等。\n1. 引入消息队列中间件 在项目的秒杀业务中，优惠券下单和数据持久化至数据库利用Redis Stream实现的消息队列解耦，优势主要在：\n简化下单流程，提高系统响应速度。 提高并发量和鲁棒性，防止数据库击穿。 1.1 为什么引入消息队列中间件？ 持久化：Redis Stream依赖AOF/RDB持久化，主从切换时异步复制可能导致数据丢失；消息队列中间件，如RocketMQ，同步刷盘+多副本（RAFT协议），提供金融级可靠性。\n消息积压：\n能力 Redis Stream 专业消息队列 存储介质 内存（成本高） 磁盘（成本低） 积压容忍度 需设置MAXLEN截断旧消息 支持TB级堆积（如Kafka） 内存风险 可能触发OOM（需手动扩内存） 磁盘空间自动扩容无压力 运维与生态：专业消息队列工具链更完善。\n高级功能：消息队列中间件引入了延迟队列、死信路由等企业级特性。\n1.2 引入哪一种？ 三种中间件对比：\n能力维度 RabbitMQ Kafka RocketMQ 吞吐量 万级 TPS 百万级 TPS 十万级 TPS 延迟 微秒级 毫秒级（批处理设计） 毫秒级 事务支持 轻量级事务（同步阻塞） 支持（≥0.11 版本） 分布式事务消息 顺序性保障 单队列有序 分区内有序 队列/分区严格有序 可靠性机制 镜像队列+持久化 多副本+ISR 同步刷盘+多副本+RAFT 协议 秒杀核心优势 削峰填谷、异步解耦 超高吞吐、日志流处理 高并发+强一致性+低延迟 结论：选择RocketMQ，秒杀场景在需要高吞吐量的同时，需要强一致性和可靠性。同时RocketMQ支撑阿里多次双十一活动，非常无敌，必须喽他。\n1.3 引入RocketMQ 1.3.1 部署RocketMQ 由于本人太穷，服务器只有2核2G，但又不想妥协用轻量级的MQ，为验证项目逻辑，在本地Windows环境部署RocketMQ。\n下载跳过。\n配置环境变量：\n1 2 %ROCKETMQ_HOME% = ...\\rocketmq-all-5.3.3-bin-release %NAMESRV_ADDR% = localhost:9876 启动NameServer和Broker：\n1 2 3 cd %ROCKETMQ_HOME% bin/mqnamesrv bin/mqbroker -n localhost:9876 测试生产消费：\n1 2 bin/tools org.apache.rocketmq.example.quickstart.Producer bin/tools org.apache.rocketmq.example.quickstart.Consumer 1.3.2 引入Java客户端依赖 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.rocketmq\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rocketmq-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 1.3.3 修改applications.yml 1 2 3 4 5 rocketmq: name-server: http://localhost:9876 producer: group: ${spring.application.name} send-message-timeout: 3000 1.4 架构更新 目前架构：\n主线程：Lua(Redis校验下单资格 -\u0026gt; 创建订单至Redis) -\u0026gt; 向异步线程阻塞队列提交订单\n子线程：异步线程获取分布式锁 -\u0026gt; 持久化至数据库(Transactional) -\u0026gt; 解锁\n其中分布式锁的设计是原本在多台Tomcat下，会导致重复下单问题。但现在Redis由于是串行化的，无论多少台Tomcat都不会出现并发问题，且MQ也将创建订单的消息串行化了，故分布式锁可以取消。\n更新后架构：\n主线程：Lua(Redis校验下单资格 -\u0026gt; 创建订单至Redis) -\u0026gt; 向MQ生产订单消息\n子线程：消费MQ消息 -\u0026gt; 持久化至数据库(Transactional)\n需要注意的是，在秒杀场景下，用户不应该为DB的错误买单，而且分布式事务违背了异步下单提高性能的初衷。故如果DB更新失败，不应该回滚Redis，而是应该重试DB更新操作。\n1.5 RocketMQ分布式事务逻辑 1.6 代码实现 1.6.1 MQ配置类 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 @Configuration public class RocketMQConfig { @Value(\u0026#34;${rocketmq.name-server}\u0026#34;) private String nameServer; // 事务消息生产者 @Bean(initMethod = \u0026#34;start\u0026#34;, destroyMethod = \u0026#34;shutdown\u0026#34;) public TransactionMQProducer transactionProducer() { TransactionMQProducer producer = new TransactionMQProducer(\u0026#34;voucher_order_group\u0026#34;); producer.setNamesrvAddr(nameServer); producer.setTransactionListener(transactionListener()); // 绑定事务监听器 return producer; } // 事务监听器实现 @Bean public TransactionListener transactionListener() { return new VoucherOrderTransactionListener(); } } 简单，跳过。\n1.6.2 订单事务监听器 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @Component public class VoucherOrderTransactionListener implements TransactionListener { @Lazy @Resource private VoucherOrderServiceImpl voucherOrderService; // 执行本地事务（订单创建） @Override public LocalTransactionState executeLocalTransaction(Message msg, Object arg) { try { VoucherOrder order = JSON.parseObject(msg.getBody(), VoucherOrder.class); voucherOrderService.createVoucherOrder(order); // 调用订单创建方法 return LocalTransactionState.COMMIT_MESSAGE; } catch (Exception e) { voucherOrderService.rollbackRedis(order.getVoucherId(), order.getUserId()); // 非数据库操作导致的回滚 return LocalTransactionState.ROLLBACK_MESSAGE; } } // 事务回查（防止本地事务未提交） @Override public LocalTransactionState checkLocalTransaction(MessageExt msg) { String orderId = msg.getKeys(); VoucherOrder order = voucherOrderService.getById(orderId); return order != null ? LocalTransactionState.COMMIT_MESSAGE : LocalTransactionState.ROLLBACK_MESSAGE; } } 这里几个关键点：\n@Lazy：由于Service中注入了transactionProducer，而其依赖配置类中创建的TransactionListener，故产生循环依赖，需要使用懒加载打破循环依赖。\n回滚：如注释。\n1.6.3 修改秒杀下单逻辑 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 @Resource private TransactionMQProducer transactionProducer; @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString() ); int r = result.intValue(); if (r != 0) { return Result.fail(r == 1 ? \u0026#34;库存不足\u0026#34; : \u0026#34;不能重复下单\u0026#34;); } // 保存order VoucherOrder order = new VoucherOrder(); order.setId(redisIdWorker.nextId(\u0026#34;order\u0026#34;)); order.setUserId(UserHolder.getUser().getId()); order.setVoucherId(voucherId); // 构造消息 Message msg = new Message(\u0026#34;voucher_order_topic\u0026#34;, JSON.toJSONBytes(order)); msg.setKeys(order.getId().toString()); try { transactionProducer.sendMessageInTransaction(msg, null); } catch (MQClientException e) { log.error(\u0026#34;MQ错误\u0026#34;); } return Result.ok(order.getId()); } @Transactional public void createVoucherOrder(VoucherOrder voucherOrder) { Long userId = voucherOrder.getUserId(); Long voucherId = voucherOrder.getVoucherId(); boolean success = seckillVoucherService.update() .setSql(\u0026#34;stock = stock - 1\u0026#34;) .eq(\u0026#34;voucher_id\u0026#34;, voucherId) .gt(\u0026#34;stock\u0026#34;, 0) // CAS乐观锁 .update(); if (!success) { // 数据库操作失败 // 1. 删除购买记录 String key = RedisConstants.SECKILL_ORDER_KEY + voucherId; stringRedisTemplate.opsForSet().remove(key, userId.toString()); // 2. 恢复预扣库存 stringRedisTemplate.opsForValue() .increment(RedisConstants.SECKILL_STOCK_KEY + voucherId, 1); throw new RuntimeException(\u0026#34;数据库扣减失败\u0026#34;); } save(voucherOrder); } //幂等Redis回滚 public void rollbackRedis(Long voucherId, Long userId) { String luaScript = \u0026#34;local orderKey = KEYS[1] \u0026#34; + \u0026#34;local stockKey = KEYS[2] \u0026#34; + \u0026#34;local userId = ARGV[1] \u0026#34; + // 只回滚存在的订单记录 \u0026#34;if redis.call(\u0026#39;sismember\u0026#39;, orderKey, userId) == 1 then \u0026#34; + \u0026#34; redis.call(\u0026#39;srem\u0026#39;, orderKey, userId) \u0026#34; + \u0026#34; redis.call(\u0026#39;incr\u0026#39;, stockKey) \u0026#34; + // 库存+1 \u0026#34; return 1 \u0026#34; + \u0026#34;else \u0026#34; + \u0026#34; return 0 \u0026#34; + // 已处理或无记录 \u0026#34;end\u0026#34;; String orderKey = RedisConstants.SECKILL_ORDER_KEY + voucherId; String stockKey = RedisConstants.SECKILL_STOCK_KEY + voucherId; // 执行Lua脚本 Long result = stringRedisTemplate.execute( new DefaultRedisScript\u0026lt;\u0026gt;(luaScript, Long.class), Arrays.asList(orderKey, stockKey), userId.toString() ); log.debug(\u0026#34;回滚结果: {}\u0026#34;, result); } ","date":"2025-06-01T00:00:00Z","permalink":"https://sixiyida.github.io/p/%E7%82%B9%E8%AF%84%E9%A1%B9%E7%9B%AE%E4%BC%98%E5%8C%96/","title":"点评项目优化"},{"content":"@Configuration和@Component @Configuration自身也是一个Bean，默认启用 CGLIB 代理（proxyBeanMethods=true）。当配置类中的 @Bean 方法相互调用时，Spring 会拦截调用并返回容器中的单例 Bean，而非重新创建，可直接注入方法参数的Bean。\n1 2 3 4 5 6 7 @Configuration public class Config { @Bean public A a() { return new A(); } @Bean public B b() { return new B(a()); } // 注入容器中的单例A } @Component无代理机制。方法间调用视为普通 Java 方法，每次调用 @Bean 方法都会创建新实例，破坏单例， 依赖注入需要显式@Autowired：\n1 2 3 4 5 6 7 @Component public class ComponentConfig { @Bean public A a() { return new A(); } @Bean public B b() { return new B(a()); } // 每次调用a()创建新实例！ } @PathVariable 1 2 3 4 @GetMapping(\u0026#34;/{id}\u0026#34;) public Result queryBlogById(@PathVariable(\u0026#34;id\u0026#34;) Long id) { } 将url的值绑定至方法参数中。\n","date":"2025-05-30T00:00:00Z","permalink":"https://sixiyida.github.io/p/spring%E7%9B%B8%E5%85%B3/","title":"Spring相关"},{"content":"为什么需要消息队列？ 在异步任务，如生产者-消费者模型中，两者的运行速度并不相同，使用消息队列可以做一个缓冲，减小系统压力。\nRedis消息队列 1. 基于List Redis的list数据结构是一个双向链表，利用LPUSH和RPOP实现，若需要阻塞，使用BRPOP实现阻塞队列效果\n缺陷：每次取出消息直接从队列中移除，造成\n无法避免消息丢失：移除后如果宕机，则消息丢失。 无法有多消费者：只能消费一次并移除，无法多次消费。 2. 基于Pub/Sub 类似ROS：\n1 2 3 SUBSCRIBE channel [channel] PUBLISH channel msg PSUBSCRIBE pattern [pattern] // 订阅匹配pattern的所有频道 缺陷：每次取出消息直接从队列中移除，造成\n不支持数据持久化：数据不在Redis（内存）中保存。 无法避免消息丢失：如上。 消息堆积有上限，超出时丢失：只在消费者处缓存，有上限。 3. 基于Stream Stream是一种为消息队列设计的数据类型。\n基本添加/读取消息 1 XADD users * name jack age 21 // 向user发{name = jack, age = 21} 1 XREAD COUNT 1 BLOCK 2000 STREAMS users $ // 读users最新的1个消息，无消息阻塞2秒 消费者组 特点：\n消息分流：队列中消息分流而不是重复消费，加快速度。 消息表示：记录最后一个被处理的消息，宕机后也能恢复。 消息确认：消息被获取后存入pending-list，必须要消费者使用XACK确认消息，才会移除。 1 2 3 4 5 6 7 8 9 10 11 XGROUP CREATE mqName groupName ID [MKSTREAM] // ID(0):第一个消息/ID($):最后一个消息 // 为mqName创建名为groupName的消费者组 XGROUP DESTROY mqName groupName XGROUP CREATECONSUMER mqName groupName consumerName XGROUP DELCONSUMER mqName groupName consumerName XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS mqName [mqName ...] ID [ID ...] //ID为获取消息起始id //\u0026#34;\u0026gt;\u0026#34; : 从未消费消息开始 //“数字”: 从pending-list中第一个消息开始 RocketMQ NameServer-Broker NameServer 是 轻量级的服务注册与发现中心，类似于分布式系统中的“电话簿”或“目录服务”。\nBroker 是消息队列中实际存储、转发消息的核心角色。它是生产者和消费者直接打交道的节点，真正处理消息的存储、查询、投递等操作。\n","date":"2025-05-28T00:00:00Z","permalink":"https://sixiyida.github.io/p/message-queue/","title":"Message Queue"},{"content":"锁 行级锁 **行级锁：**在select for update等场景，即当前读（另一种是快照读MVCC）场景使用。\n包括Record Lock, Gap Lock 和 Next-key Lock(前两种的合并)\n包括X(Exclusive)和S(share)两种\nGap Lock是只锁相邻两条记录之间的()，NK-lock是锁(]\nGap Lock的X和S型是一样的，都可以重复获取，NK-lock要看右区间的记录锁是否互斥，无限除外。\n怎么加？（MySQL8.0.26, 可重复读） 对索引加，基本单位是nk-lock，不同情况可能出现退化为前两种\n主键索引等值查询：\n记录存在-\u0026gt;退化为记录\n记录不存在-\u0026gt;退化为间隙锁\n1 select * from performance_schema.data_locks //查加了什么锁 如果MODE是GAP， LOCK_DATA是右区间界。\n主键索引范围查询：\n大于：不退化\n大于等于：如果等于存在，则左边退化为记录锁，不存在则不退化\n小于：最右侧退化为间隙锁\n小于等于：若等于存在，则不退化，不存在则最右侧退化为间隙锁\n**注：**记录锁属于记录，在GAP锁中是属于LOCK_DATA，右区间界的记录。\n二级索引（非唯一）等值查询：\n记录不存在：二级索引上GAPLOCK，对于左右端点，能否插入要看二级索引B+树下一条记录有无GAPLOCK\n记录不存在的特殊情况：如果是超过了最大id，是next-keyLock\n注：二级索引GAPLOCK的LOCK_DATA包含两个值，二级索引和回表的主键索引\n记录存在：对二级索引匹配的记录加nk锁，对不匹配的第一个nk锁退化为间隙锁，且在主键索引加记录锁。\n对于端点是否能插入，和要看二级索引B+树下一条记录有无LOCK\n在不匹配的第一个索引加nk锁的目的：防止幻读（id \u0026gt; lock_id）的情况\n二级索引（非唯一）范围查询：不退化，二级的nk和主键的记录锁都加\n如果不走索引全表扫描，则所有记录全部加nk锁，全锁，是事故\n解决方案：将sql_safe_updates设置为1，此时必须使用where+索引 / limit\n当前读的语句：update、delete、select for update，会加意向锁和行锁\n死锁的形成 例子：如果两个事务都获取了间隔锁，且希望插入对方间隔，则尝试获取插入意向锁（和间隔锁互斥），环路等待导致死锁。\ninsert语句加行级锁 **记录之间有间隙锁：**加插入意向锁\n注：mysql的锁是先生成锁结构，锁此时是等待状态，再获取锁，如果不能获取则阻塞。\n**唯一键（主键或唯一二级索引）冲突：**失败后加S型锁\n主键：加S记录锁\n唯一二级索引：加S型NK锁\n例子：在select for update中，尝试加X型锁，和S型冲突，所以失败。\n并发insert导致的唯一键冲突\n第一个insert但事务没提交时，构造隐式锁。\n第二个insert时，隐式锁变成X型锁，和第二个insert想要获取的S型nk锁冲突\n避免死锁的方法 设置事务等待回滚时间：超时回滚\n1 innodb_lock_wait_timeout = 50 // default 开启主动死锁检测：主动回滚\n1 innodb_deadlock_detect = on 日志 保证ACID特性：Atomic, Consisitency, isolation, duration\nundo log 注：对于增删改语句，innodb会隐式启动事务。\n特殊处理：delete只在记录上打标记，真正删除由purge线程完成\nupdate非主键列：直接update，且在undolog中记录update之前的值\nupdate主键列：先删再插\nundolog的存储形式：由roll_pointer指针形成链表穿起来\nbufferpool 指的是innodb引擎中的内存bufferpool。\n类似于pagecache，由后台线程实现脏页写回机制。\n内存结构：首先申请连续的内存空间，接着按照16kb大小划分出缓存页。\n包括数据页、索引页、undo页、插入缓存页、自适应哈希索引、锁信息等。\nredolog **redolog的作用：**对修改实现持久化。\nWAL技术(write-ahead logging)：在写入磁盘之前先写入redolog。\n每对Bufferpool进行修改就写入redolog，包括undolog的修改\nredolog和undolog对比：\nredolog是记录修改后，保证持久化\nundolog是记录修改前，保证原子化\n写入数据和写入redolog对比：\nredolog：是顺序写入，高效\n数据：是随机写入，低效\nredolog也有buffer，落盘时机：\nmysql正常关闭，buffer空间超过一半，每一秒写回一次。\ninnodb_flush_log_at_trx_commit参数：提交事务时的行为\n0：不写回，后台线程每隔一秒用write()和fsync()\n1：直接持久化到磁盘\n2：写入文件（pagecache）由操作系统写回，后台线程每隔一秒用fsync()\nredolog存储方式：两个redolog文件循环存储类似于环形队列。\n有个tail和head，在tail处写，持久化bufferpool进入数据后更新head。\nbinlog Server层的日志， 用于备份恢复、主从复制。三种格式类型：\nSTATEMENT：记录SQL语句逻辑操作\nROW：记录行数据最终修改情况\nMIXED：根据情况使用STATEMENT或者ROW\n使用追加写，写满文件就创建新文件继续写，全量日志。\n主从复制 主库server层直接写入binlog，后台log dump线程异步将binlog日志发给从库，从库relaylog记录binlog，后台线程异步执行relaylog。\n**从库的数量选择：**对主库的资源消耗、网络带宽。\n其他模型：\n同步模型：要所有从库relaylog记录完毕后返回成功，主库再返回客户端。没法用\n异步模型：默认模型，主库宕机就gg\n半同步：一部分库返回成功即可。\nbinlog也有cache，每个线程各一个。\n持久化的时机：\nbinlog_cache_size：超过这个大小就写入\nsync_binlog：\n0：只write，操作系统控制写回\n1：write+fsync\nn：write，累积n个以后fsync\n注：binlog在语句执行完成后在记录。事务提交时候才持久化。\n两阶段提交 问题：如果redolog和binlog一个完成一个不完成，则会出现主从不一致的问题。\n内部XA事务：在事务提交后开启，由binlog协调。\n将redolog写入拆为prepare和commit，中间插入binlog持久化。\nprepare：将内部XID写入redolog并持久化，将状态设置为prepare。\ncommit：将内部XID写入binlog并持久化，接着将redolog设置为commit。\n崩溃时，redolog处于prepare状态，检查binlog中有无XA事务的id，有则提交事务，无则回滚。\n**问题：**性能差\n磁盘IO次数高\n在多事务下，不能保证两者提交顺序一致，需要加锁以保证提交的原子性\n上述问题：binlog组提交机制\n分为flush(write)、sync(fsync)、commit阶段，每个阶段都有队列，用锁保证事务写入顺序。\nredolog的组提交机制\n将redolog刷盘延迟到flush阶段中\nBufferPool default = 128MB\neach page 16kb\n结构：控制块1到n，接着page1到n\n注：查询时候直接将innodb的整个页加载至bufferpool中，然后在bufferpool中通过页目录定位记录\nFREE链表：管理空闲页，节点是控制块，头结点包括链表头尾地址和控制块数量。\nFLUSH链表：管理空闲页，节点是控制块，头结点包括链表头尾地址和控制块数量。\n如何管理bufferpool：\n传统LRU问题：\n**预读失效：**预读时会把相邻的数据页一并加载，为了减少磁盘IO，如果这些没有被访问，且淘汰末尾页，则降低缓存命中率。\n**解决方法：**划分LRU的优先级，前面是YOUNG，后面是OLD，预读先加入OLD，真正访问才加入YOUNG区域。\n**缓存污染：**当扫描大量数据，会淘汰大量热数据，导致命中率下降。\n**解决方法：**提高加入YOUNG的门槛，记录第一次OLD被访问的时间，如果后续访问时间超过第一次1s，则放入young区域。\n**Linux的做法：**第二次的时候将才升级到active，比MYSQL简单。\n","date":"2025-05-28T00:00:00Z","permalink":"https://sixiyida.github.io/p/mysql/","title":"MySQL"},{"content":"为什么需要分布式锁？ 在单JVM环境中，对于一人一单的场景，可以使用互斥锁实现。但是在负载均衡的集群场景中，需要全局锁，即分布式锁。\n常见的分布式锁实现方式：MySQL、Redis、Zookeeper。\nMySQL分布式锁实现方式：\n1.利用唯一索引，插入唯一键值成功则获取锁，释放锁则直接删除该记录。\n2.利用MySQL排他锁（SELECT FOR UPDATE），提交事务时释放锁。\nRedis分布式锁 加锁 1 SET lock thread1 NX EX 10 1 Boolean success = stringRedisTemplate.opsForValue().setIfAbsent(KEY_PREFIX + name, threadId + \u0026#34;\u0026#34;, timeoutSec, TimeUnit.SECONDS); 解锁 1 DEL lock 1 stringRedisTemplate.delete(KEY_PREFIX + name); 误删问题 由于可能出现线程阻塞超时自动释放，且锁在当前线程恢复之前被其他线程获取，该线程恢复之后如果直接释放锁，会释放其他线程获取的分布式锁，出现混乱。\n解决方法：\n判断锁所有权，再删除。问题：当判断所有权之后如果线程阻塞，同样会出现上述问题。 将1中操作变成原子的，使用Redis提供的Lua脚本。 1 2 3 4 if (redis.call(\u0026#39;get\u0026#39;, KEYS[1]) == ARGV[1]) then return redis.call(\u0026#39;del\u0026#39;, KEYS[1]) end return 0 1 2 3 4 5 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), threadId ); 基于setnx实现的分布式锁的问题 不可重入：同线程无法多次获取同一把锁，可能会导致同线程不同方法相互依赖导致死锁。 不可重试：获取锁失败没有重试机制 超时释放：执行时间长可能导致锁意外自动超时释放。 主从一致性：加锁后主节点宕机，从节点未同步，导致重复获取锁。 Redisson 基于Redis实现的分布式工具。\nStep0：Maven添加依赖\n1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.redisson\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;redisson\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.22.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; **Step1：**注入RedissonClient\n1 2 3 4 5 6 7 8 9 10 11 @Configuration public class RedissonConfig { @Bean public RedissonClient redissonClient(){ Config config = new Config(); config.useSingleServer().setAddress(\u0026#34;redis://localhost:6379\u0026#34;).setPassword(\u0026#34;password\u0026#34;); return Redisson.create(config); } } **Step2：**使用工具\n1 RLock lock = redissonClient.getLock(RedisConstants.ORDER_LOCK_KEY + userId); Redisson分布式锁原理 基于Lua脚本的可重入 利用hash结构，记录线程id和引用次数。\n加锁：判断是否为当前线程id，如果是，则引用计数+1，重置锁有效期。\n解锁：判断是否为当前线程id，如果不是，不用处理；如果是，则引用计数-1并重置锁有效期。最后判断如果引用计数为0，则释放锁。\n可重试 在解锁时使用信号量/Pub，通知解锁。\n加锁失败后若有剩余等待时间，收到解锁信号后，异步重试。\n锁续期 加锁成功后启动后台线程Watchdog，每10秒检查锁是否被持有，若持有则续期为30秒。\nMultiLock 1 Rlock lock = redissonClient.getMulitLock(lock1, lock2, ...); 原理：\n获取锁：遍历获取 + 失败回滚\n建立锁List，遍历获取锁，遍历时分配每个锁的等待时间。\n若出现获取失败，则释放所有已经获取的锁，并返回失败。\n如果所有锁获取成功则返回成功。\n解锁：遍历释放 + 异常容忍\n遍历所有锁并逐一释放（无论是否属于当前线程）。\n即使某个锁释放失败（如锁已超时），仍继续释放其他锁，最大限度避免死锁。\n","date":"2025-05-28T00:00:00Z","permalink":"https://sixiyida.github.io/p/redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","title":"Redis分布式锁"},{"content":"静态代码块初始化 1 2 3 4 5 static { UNLOCK_SCRIPT = new DefaultRedisScript\u0026lt;\u0026gt;(); UNLOCK_SCRIPT.setLocation(new ClassPathResource(\u0026#34;unlock.lua\u0026#34;)); UNLOCK_SCRIPT.setResultType(Long.class); } 在类首次加载时执行一次，如通过new创建对象、访问静态成员或反射加载类时触发\n多个静态代码块按定义顺序依次执行\n可以进行复杂逻辑初始化\n非静态成员变量初始化 在对象创建时完成，顺序为声明赋值 → 初始化块 → 构造函数，每种与代码书写顺序一致。\nSpringBoot中的classpath 一句话总结：classpath 等价于 main/java + main/resources + 第三方jar包的根目录。\n1 UNLOCK_SCRIPT.setLocation(new ClassPathResource(\u0026#34;unlock.lua\u0026#34;)); ClassPathResource对应main/resources目录下文件。\nAOP的内部调用问题 在使用AOP的场景中，如@Transactional，如果使用this指针调用内部方法，会绕过代理导致AOP失效。\n解决方法：\n使用AopContext.currentProxy()直接获取当前代理对象，原理是通过 ThreadLocal 存储当前线程的代理对象。 缺陷：在多线程场景下，如果子线程调用父线程的事务函数，由于ThreadLocal不互通，导致无法获取代理对象，事务失效。\n缺陷的解决方法：直接在主线程中将获取的代理对象传给子线程任务。\n直接使用@Autowired将自身注入。 **缺陷：**循环依赖风险，三级缓存性能低。\nRESTful API：PUT 更新或创建指定位置的资源。客户端需提供完整的资源数据，服务器会完全替换目标 URI 对应的资源。若资源不存在，则新建资源。\n与POST对比：\n特性 PUT POST 幂等性 ✅ 是（多次请求结果一致） ❌ 否（可能产生多个资源） URI 含义 资源唯一标识（如 /users/123） 资源集合（如 /users） 数据完整性 必须提供完整资源 可提交部分数据 典型响应码 200 OK（更新）或 201 Created（新建） 201 Created（新建资源） @Resource和@Autowired对比 @Resource流程\n指定name → 按名称查找 → 失败抛异常\n未指定name → 先按字段名匹配 → 失败则按类型匹配\n指定type → 按类型唯一匹配 → 多匹配抛异常\n@Autowired 流程\n按类型查找 → 找到唯一 Bean → 注入成功\n找到多个同类型 Bean → 需结合 @Qualifier(\u0026quot;beanName\u0026quot;) 指定名称\n无匹配且→ 注入required=false → 注入 null\no instanceof Node node的用法（Java 16+） 等于以下代码：\n1 2 3 if (o instanceof Node) { Node node = (Node) o; } record（Java 16+） 1 public record Person(String name, int age) {} 自动生成内容：\n全参构造器（如Person(String name, int age)） 字段访问器（如name()、age()，而非传统getName()） equals()、hashCode()、toString()方法 所有字段默认为final，实例化后不可修改\nthis() 调用其他构造函数 1 2 3 4 5 6 public record Edge(String sourceId, List\u0026lt;EdgeValue\u0026gt; targets) { // 构造函数 public Edge(String sourceId, EdgeValue target) { this(sourceId, List.of(target)); } } PECS原则 Producer Extends, Consumer Super.\n1 private static List\u0026lt;? extend K\u0026gt; produce() 原因：PE可以保证获取的至少是一个K，这样获取出来的可以直接用K来接，是类型安全的。\n1 private static consume(List\u0026lt;? super U\u0026gt;, U) 原因：CS可以保证接收消费对象的容器装的是其父类对象，可以保证传入的对象可以被向上转型，是类型安全的。\n自定义Collector Collector接口定义了5个核心方法，需全部实现：\nsupplier() 创建结果容器（如ArrayList::new），用于存储中间结果\naccumulator() 定义如何将元素添加到容器（如List::add），处理单个元素的累加逻辑\ncombiner() 合并并行流的子结果（如合并两个List：list1.addAll(list2)），需保证线程安全\nfinisher() 将中间容器转换为最终结果（如StringBuilder::toString），可进行最终转换或过滤\ncharacteristics() 返回收集器特性的Set，影响性能优化：\nCONCURRENT：支持多线程并发操作容器（需线程安全）\nUNORDERED：结果与元素顺序无关（如Set）\nIDENTITY_FINISH：跳过finisher()，直接返回中间容器\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 private static \u0026lt;T, K, U\u0026gt; Collector\u0026lt;T, ?, Map\u0026lt;K, U\u0026gt;\u0026gt; toMapRemovingNulls(Function\u0026lt;? super T, ? extends K\u0026gt; keyMapper, Function\u0026lt;? super T, ? extends U\u0026gt; valueMapper, BinaryOperator\u0026lt;U\u0026gt; mergeFunction) { return Collector.of(HashMap::new, (map, element) -\u0026gt; { K key = keyMapper.apply(element); U value = valueMapper.apply(element); if (value == null) { map.remove(key); } else { map.merge(key, value, mergeFunction); } }, (map1, map2) -\u0026gt; { map2.forEach((key, value) -\u0026gt; { if (value != null) { map1.merge(key, value, mergeFunction); } }); return map1; }, Collector.Characteristics.UNORDERED); } Optional对象 将可能为 null 的对象包装在 Optional 容器中，强制开发者显式处理空值场景。\n1 Optional\u0026lt;String\u0026gt; name = Optional.ofNullable(getName()); 提供 map(), flatMap(), filter() 等方法，支持以函数式风格处理值：\n1 2 3 4 5 // 链式获取嵌套属性（避免多层判空） String province = Optional.ofNullable(user) .map(User::getAddress) // 若user非空，提取地址 .map(Address::getProvince) // 若地址非空，提取省份 .orElse(\u0026#34;未知地区\u0026#34;); // 若为空，返回默认值 ","date":"2025-05-28T00:00:00Z","permalink":"https://sixiyida.github.io/p/%E6%97%A5%E5%B8%B8%E9%97%AE%E9%A2%98%E9%9B%86/","title":"日常问题集"},{"content":"线段树 核心思想：分治\n节点上维护[l, r]的某个值，左儿子节点维护[l, mid]，右儿子节点维护[mid + 1, r]。\n例题：LeetCode3479\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 class SegmentTree{ vector\u0026lt;int\u0026gt; mx; void maintain(int o){ mx[o] = max(mx[o * 2], mx[o * 2 + 1]); } void build(const vector\u0026lt;int\u0026gt; \u0026amp; a, int o, int l, int r) { if (l == r) { mx[o] = a[l]; return; } int m = (l + r) / 2; build(a, o * 2, l, m); build(a, o * 2 + 1, m + 1, r); maintain(o); //更新的关键操作 } public: SegmentTree(const vector\u0026lt;int\u0026gt; \u0026amp; a) { size_t n = a.size(); mx.resize(2 \u0026lt;\u0026lt; bit_width(n - 1));//? build(a, 1, 0, n - 1); } int findFirstAndUpdate(int o, int l, int r, int x) { if (mx[o] \u0026lt; x) { return -1; } if (l == r) { mx[o] = -1; return l; } int m = (l + r) /2; int i = findFirstAndUpdate(o * 2, l, m, x); if (i \u0026lt; 0) i = findFirstAndUpdate(o * 2 + 1, m + 1, r, x); maintain(o); return i; } }; Lazy线段树 为什么lazy？ ​\t待节点区间完全在需要更新的区间内时，则不继续向下更新，而是标为lazy标记，待下一次需要更新到子节点的时候，再把这个标记向下传递。\n例题：LeetCode2569\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class LazySegmentTree{ private: vector\u0026lt;int\u0026gt; cnt; vector\u0026lt;int\u0026gt; todo; void maintain(int o) { cnt[o] = cnt[o * 2] + cnt[o * 2 + 1]; } void build(const vector\u0026lt;int\u0026gt; \u0026amp; a, int o, int l, int r) { if (l == r) { cnt[o] = a[l]; return; } int mid = (l + r) / 2; build(a, o * 2, l, mid); build(a, o * 2 + 1, mid + 1, r); maintain(o); } void reverse(int o, int l, int r) { cnt[o] = r - l + 1 - cnt[o]; todo[o] = !todo[o]; } public: LazySegmentTree(const vector\u0026lt;int\u0026gt; \u0026amp; a) { int n = a.size(); cnt.resize(4 * n); todo.resize(4 * n); build(a, 1, 0, n - 1); } void update(int o, int l, int r, int L, int R) { if (L \u0026lt;= l \u0026amp;\u0026amp; r \u0026lt;= R) { //cout \u0026lt;\u0026lt; l \u0026lt;\u0026lt; \u0026#34; \u0026#34; \u0026lt;\u0026lt; r \u0026lt;\u0026lt; endl; reverse(o, l, r); return; } int m = (l + r) / 2; if (todo[o]) { reverse(o * 2, l, m); reverse(o * 2 + 1, m + 1, r); todo[o] = false; } if (m \u0026gt;= L) { update(o * 2, l, m, L, R); } if (m \u0026lt; R) { // m + 1 \u0026lt;= R update(o * 2 + 1, m + 1, r, L, R); } maintain(o); } int getRootVal() { return cnt[1]; } }; KMP算法 核心思想：主串指针不动，子串动。子串从next[sub_ptr]的位置启动，next的含义是从[0, fail_sub_ptr]的子串中，相同的最长真前后缀长度。\n","date":"2025-04-28T00:00:00Z","permalink":"https://sixiyida.github.io/p/algorithms/","title":"Algorithms"}]